---
title: "GLM Part3"
author: "Murray Logan"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    collapse: no
    df_print: paged
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: textmate
    theme: spacelab
    toc: yes
    toc_float: yes
    css: ../resources/style.css
  pdf_document:
    df_print: default
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: tango
    latex_engine: xelatex
    number_sections: yes
    toc_depth: 2
  word_document:
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: tango
    toc: yes
    toc_depth: 2
output_dir: "docs"
documentclass: article
fontsize: 12pt
mainfont: Arial
mathfont: LiberationMono
monofont: DejaVu Sans Mono
classoption: a4paper
bibliography: ../resources/references.bib
---

```{r setup, include=FALSE, warnings=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preparations

Load the necessary libraries

```{r libraries, results='markdown', eval=TRUE, message=FALSE, warning=FALSE}
library(car)       #for regression diagnostics
library(broom)     #for tidy output
library(ggfortify) #for model diagnostics
library(sjPlot)    #for outputs
library(knitr)     #for kable
library(effects)   #for partial effects plots
library(emmeans)   #for estimating marginal means
library(MASS)      #for glm.nb
library(MuMIn)     #for AICc
library(tidyverse) #for data wrangling
```

# Scenario

Here is a modified example from @Peake-1993-269.  @Peake-1993-269 investigated the relationship between the number of individuals of invertebrates living in amongst clumps of mussels on a rocky intertidal shore and the area of those mussel clumps.

![](../resources/mussels.jpg)

Format of peakquinn.csv data files

| AREA      | INDIV   |
| --------- | ------- |
| 516.00    | 18      |
| 469.06    | 60      |
| 462.25    | 57      |
| 938.60    | 100     |
| 1357.15   | 48      |
| \...      | \...    |

----------- --------------------------------------------------------------
**AREA**    Area of mussel clump mm^2^ - Predictor variable
**INDIV**   Number of individuals found within clump - Response variable
----------- --------------------------------------------------------------



The aim of the analysis is to investigate the relationship between mussel clump area and the number of non-mussel invertebrate individuals supported in the mussel clump.

# Read in the data

```{r readData, results='markdown', eval=TRUE}
library(readr)
peakquinn <- read.csv("~/Desktop/AIMS/SUYR/data/peakquinn.csv")
```


# Exploratory data analysis

Model formula:
$$
y_i \sim{} \mathcal{Pois}(\lambda_i)\\
ln(\lambda_i) = \beta_0 + \beta_1 ln(x_i)
$$


# Fit the model
```{r}
ggplot(peakquinn,aes(AREA, INDIV)) + geom_point()
#the ggplot suggests heterogeneity of variance

peakquinn.lm =lm(INDIV~AREA, data = peakquinn)
plot(peakquinn.lm)
```


## Candidate models {.tabset .tabset-faded}
We try log transforming both axis
```{r}
ggplot(peakquinn, aes(y = INDIV, x = AREA)) +geom_point() + scale_x_log10() +scale_y_log10() + geom_smooth()
```

#Exploratory data analysis
###LM
```{r}
peake.lm1 = lm(INDIV~log(AREA), data = peakquinn)
plot(peake.lm1)
#the residual plot is awful. as you move from left to right, data is more spread out. 
```
```{r}
peake.lm2 = lm(log(INDIV)~log(AREA), data = peakquinn)
plot(peake.lm2)
#the residual plot is better. the qqplot shows that we haven't normalised our data fully.
#OLS models are fairly robust to nonnormality, provided variance is okay. this fit might be acceptable.
```

### Poisson (GLM)
```{r}
peake.glm = glm(INDIV~log(AREA), data = peakquinn, family = poisson())
plot(peake.glm)
#the residual plot suggests that the mean doesn't equal variance.
```

#### Model validation
Explore a lack of fit
- No homogeneity of variance
- Non-normality
- Large Cook's D
Theory however suggests that it would be a sensible plot to use...

```{r}
#GOODNESS OF FIT TEST
1-pchisq(peake.glm$deviance, peake.glm$df.residual) #there is evidence for lack of fit 0<0.05
```

#Explore overdispersion 
- Dispersion = Variance / Mean
  #OR Deviance/DF of residuals
- In Poisson: Dispersion = 1
```{r}
peake.glm$deviance/peake.glm$df.residual #our dispersion value is way larger than 1
#A dispersion greater than 3, you indicate that the model is overdispersed.
```


### Negative Binomial (GLM)
```{r}
library(MASS)
peake.neg = glm.nb(INDIV~log(AREA), data = peakquinn)
autoplot(peake.neg, which = 1:6) 
#cooks d values look much better
```

#### Model validation

Explore a lack of fit
```{r}
1-pchisq(peake.neg$deviance,peake.neg$df.residual)
```

Explore overdispersion
```{r}
peake.neg$deviance/peake.neg$df.residual
```

## Comparisons (model selection)
- Complexity (Simple > Complex)
- Goodness of fit
We need to use a metric : AIC
```{r}
AIC(peake.lm2, peake.glm, peake.neg)
#the lower the values the better
#For likelihood, the smaller the value, the better the log
```
According to AIC, the first model is the best model with the lowest AIC. 
The most parsimmonous model, is the log-log model. However this model doesn't represent the data. We choose negbin, because of the better theoretical ground, despite its added complexity (higher df).

Because of the sample size is small, we use corrected AIC:
```{r}
library(MuMIn)
AICc(peake.lm2, peake.glm, peake.neg)
```


# Model investigation / hypothesis testing
```{r}
plot(allEffects(peake.neg, residuals = T), type = 'response')
#the axis are displayed in the scale of our response, not log scale.
```
Residuals = Observed - Expected
Observed = Expected + Residuals
- For plotting effect size, we plot standardized residuals. 

# Predictions

```{r}
summary(peake.neg)
```
The coefficient are always on the scale of your link function, log in this case.

```{r}
exp(coef(peake.neg))
```
For every 1 unit change in log AREA, the response is multiplied by 2.2811753.
The curve of AREA and INDIV is not a straight line, but exponential.

#Strength of the relationship
```{r}
1-(peake.neg$deviance/peake.neg$null) #R squared
```
Our model is explaining about 84% of the variability of individuals, with respect to the area they live in.

```{r}
r.squaredLR(peake.neg) #different way of calculating R squared
``` 
A null model will have all the variance unexplained. Then it corrects for your sample size. 
Because R squared can't ever go beyond 1, the adjusted R squared will take this into account. This suggests we're explaining about 85.5%. This is the best value to use.

#Summary Figures
```{r}
peake.grid = with(peakquinn, list(AREA = seq(min(AREA), max(AREA), len = 100)))
newdata = emmeans(peake.neg, ~AREA, at = peake.grid, type = 'response') %>%
  as.data.frame()
newdata

```
```{r, warning= FALSE}
ggplot(newdata, aes(AREA, response)) +
  geom_ribbon(aes(ymin = asymp.LCL, ymax = asymp.UCL), fill = 'blue', alpha = 0.3) + geom_line() + theme_classic() +
  geom_point(data = peakquinn, aes(AREA, INDIV))+
  annotate(geom = 'text', x = 1000, y = 1500, label = expression(R^2==0.85), hjust=0) +
  scale_x_log10(expression(Mussel~clump~area~(mm^2))) +
  scale_y_log10('Number of individuals') 
```
Natural data on a log scale

# References
