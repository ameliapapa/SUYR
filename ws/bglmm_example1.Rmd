---
title: "Bayesian GLMM Part1"
author: "Murray Logan"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    collapse: no
    df_print: paged
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: textmate
    theme: spacelab
    toc: yes
    toc_float: yes
    css: ../resources/style.css
  pdf_document:
    df_print: default
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: tango
    latex_engine: xelatex
    number_sections: yes
    toc_depth: 2
  word_document:
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: tango
    toc: yes
    toc_depth: 2
output_dir: "docs"
documentclass: article
fontsize: 12pt
mainfont: Arial
mathfont: LiberationMono
monofont: DejaVu Sans Mono
classoption: a4paper
bibliography: ../resources/references.bib
---

```{r setup, include=FALSE, warnings=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preparations

Load the necessary libraries

```{r libraries, results='markdown', eval=TRUE, message=FALSE, warning=FALSE}
library(rstanarm)   #for fitting models in STAN
library(coda)       #for diagnostics
library(bayesplot)  #for diagnostics
library(rstan)      #for interfacing with STAN
library(emmeans)    #for marginal means etc
library(broom)      #for tidying outputs
library(tidybayes)  #for more tidying outputs
library(ggeffects)  #for partial plots
library(tidyverse)  #for data wrangling etc
```

# Scenario

A plant pathologist wanted to examine the effects of two different strengths of tobacco virus on the number of lesions on tobacco leaves.  She knew from pilot studies that leaves were inherently very variable in response to the virus.  In an attempt to account for this leaf to leaf variability, both treatments were applied to each leaf.  Eight individual leaves were divided in half, with half of each leaf inoculated with weak strength virus and the other half inoculated with strong virus.  So the leaves were blocks and each treatment was represented once in each block.  A completely randomised design would have had 16 leaves, with 8 whole leaves randomly allocated to each treatment.  

![Tobacco plant](../resources/TobaccoPlant.jpg){height="300"}

Format of tobacco.csv data files

LEAF   TREAT    NUMBER
------ -------- --------
1      Strong   35.898
1      Week     25.02
2      Strong   34.118
2      Week     23.167
3      Strong   35.702
3      Week     24.122
\...   \...     \...

------------ ----------------------------------------------------------------------------------------------------
**LEAF**     The blocking factor - Factor B
**TREAT**    Categorical representation of the strength of the tobacco virus - main factor of interest Factor A
**NUMBER**   Number of lesions on that part of the tobacco leaf - response variable
------------ ----------------------------------------------------------------------------------------------------


# Read in the data

```{r readData, results='markdown', eval=TRUE}
tobacco = read_csv('../data/tobacco.csv', trim_ws=TRUE)
glimpse(tobacco)
```


# Exploratory data analysis

Model formula:
$$
y_i \sim{} \mathcal{N}(\mu_i, \sigma^2)\\
\mu_i =\boldsymbol{\beta} \bf{X_i} + \boldsymbol{\gamma} \bf{Z_i}
$$

where $\boldsymbol{\beta}$ and $\boldsymbol{\gamma}$ are vectors of the fixed and random effects parameters respectively 
and $\bf{X}$ is the  model matrix representing the overall intercept and effects of the treatment on the number of lesions.
$\bf{Z}$ represents a cell means model matrix for the random intercepts associated with leaves.


We're interested in measuring variance. There's effects (fixed) and varying effects (random).


```{r}
tobacco = tobacco %>% mutate(LEAF = factor(LEAF), TREATMENT = factor(TREATMENT))
```

# EDA

```{r}
ggplot(tobacco, aes(y = NUMBER, x = TREATMENT)) + geom_boxplot() 
ggplot(tobacco, aes(LEAF, NUMBER, linetype = TREATMENT, group = TREATMENT)) + geom_line()
```

Random intercept would be saying:
- The first group is Strong (alphabetical). We expect it to have a higher effect than Weak. But this is not always the case.

Random slopes -> the effect is consistent in each plot. There's at least 2/8 leaves that don't have the same degree of effect as the others (L4 & L6). Instead we estimate the effect separately for each leaf, then we average them together.

What we expect from our model:
- Gaussian -> mu = beta0 + beta1*x where x=TREATMENT, beta0 is the mean of the strong group, and beta1 is the difference between the mean of both groups.
- 

# Fit the model
```{r, cache=TRUE}
tobacco.rstan = stan_glmer(NUMBER ~ (1|LEAF) + TREATMENT, data = tobacco,
                           family = gaussian, refresh = 0, chains = 3,
                           iter = 5000, warmup = 500, thin = 5)
```

We need to put priors on the two estimators, sigma (variance of each observation, and covariance, degree of correlation between each observation)

We don't care how different one leaf is from another, we only care about the effect, thus how variable the leaves are from each other.

```{r}
prior_summary(tobacco.rstan)
```

Covariance is calculated for the sigma of "random" effects. There is a correlation between Weak and Strong in Leaf 1. So we can't just use the exponential. 

# Model validation

```{r}
stan_trace(tobacco.rstan)
#We might only be interested in effect of intercept and Treatment, but because its a random intercept model, it gives us the effect of each leaf.
colnames(as.matrix(tobacco.rstan))
#These are all our parameters. sigma with small s is the overall variance, and sigma with big S is the variance of our intercept (remember random intercept model).
```

```{r}
nms = colnames(as.matrix(tobacco.rstan)) #I'm storing these names
#Now we want something to say we only want the first, second, 11th and 12th.
```

```{r}
wch = grep('^.Intercept|^TREATMENT|^[sS]igma',nms) #Pattern matching (1971)
#We're looking for something that starts with (^) and any character (.) Intercept
#OR (|) it must start with the word TREATMENT, OR (|) it must start (^) with [sS] small or big S
```

```{r}
nms[wch]
```

Now we ask stan_trace to plot only what we're interested in

```{r}
stan_trace(tobacco.rstan, pars = nms[wch])
```

```{r}
stan_ac(tobacco.rstan, pars = nms[wch]) #they look good
```

```{r}
stan_ess(tobacco.rstan, pars = nms[wch])
```

```{r}
stan_rhat(tobacco.rstan, pars = nms[wch])
```


```{r}
posterior_vs_prior(tobacco.rstan, color_by = 'vs', group_by = T, 
                   facet_args = list(scales = 'free_y'), pars = nms[wch])
```

# Model investigation / hypothesis testing

We try fitting random slope model:
```{r}
tobacco.rstan1 = stan_glmer(NUMBER~(TREATMENT|LEAF) + TREATMENT, data = tobacco,
                            family = gaussian, refresh = 0,
                            chains = 3, iter = 5000, warmup = 1000, thin = 5)
#After increasing the warmup from 500 to 1000, we got less divergent transitions.
#A small number is not a big problem. They don't impact the estimates, it is just a measure of how ineffieciently the model is running.
#Another way to remove this issue is to have stronger priors.
```

```{r}
nms = colnames(as.matrix(tobacco.rstan1))
nms
```

```{r}
wch = grep('^.Intercept|^TREATMENT|^[sS]igma',nms)
stan_trace(tobacco.rstan1, pars = nms[wch])
```

```{r}
stan_ac(tobacco.rstan1, pars = nms[wch])
#If we have more complicated models (more complicated likelihoods), it will use smaller jumps, and the autocorrelation will be higher. I probably could thin it slightly more
```

```{r}
loo(tobacco.rstan)
```

```{r}
loo(tobacco.rstan1) #There is no support for the more complex model, smaller looic
```


```{r}
compare_models(waic(tobacco.rstan), waic(tobacco.rstan1))
```

# Residual plot

```{r}
mu = fitted(tobacco.rstan)
res = resid(tobacco.rstan)
ggplot(data = NULL) + geom_point(aes(mu,res))
 #OR
augment(tobacco.rstan) %>% ggplot() + geom_point(aes(y = .resid, x= .fitted))
```

# Predictions

```{r}
ggpredict(tobacco.rstan) %>% plot
newdata = emmeans(tobacco.rstan, ~TREATMENT) %>% as.data.frame
newdata
```

```{r}
ggplot(newdata, aes(y = emmean, x = TREATMENT)) + geom_pointrange(aes(ymin = lower.HPD, ymax = upper.HPD))
```

```{r}
summary(tobacco.rstan)
#We're interested in the variance of the LEAF's : Sigma[LEAF:(Intercept),(Intercept)]  13.6
#Within LEAF variability: sigma--> 4.5
#There is evidence that there is twice as much variability between the leaves, rather than within the LEAF
```


```{r}
tidyMCMC(tobacco.rstan$stanfit, conf.int = T, conf.method = 'HPDinterval', rhat = TRUE, ess = TRUE, estimate.method = 'median')
#TREATMENTWeak	-7.71337412	2.272961	-1.244606e+01	-3.344174	
#On average, the treatment effect size is a decrease in 7.7. It can decrease from -12.4 to as little as -3.34
```

# R squared

R squared due to treatment alone:
```{r}
bayes_R2(tobacco.rstan, re.form = NA) %>% median_hdi
#We're 95% sure that we can explain 11 to 58% of the variability with the treatments alone. It's taking away the random effects (re.form = NA).
```

```{r}
bayes_R2(tobacco.rstan, re.form = ~(1|LEAF)) %>% median_hdi
#If we take into account the random effects, our model explains 24-84% of the variability
```


```{r}
newdata = emmeans(tobacco.rstan, ~TREATMENT) %>% #it is calculating the marginal means for the treatments, it is pulling the values from the individual LEAVES
  gather_emmeans_draws() %>%                     #it's gathering all of the 2700x2 emmeans for both Strong and Weak into a long format
  spread(key = TREATMENT, value = .value)        #now we've gone from long to wide. it needs to know which variable will be split up in new columns. I want a column called strong, and what Weak. It also needs to know what values to put in these columns.
newdata
```

```{r}
newdata = newdata %>% mutate(Eff = Strong-Weak, PEff = 100*(Strong-Weak)/Weak) #creates two new columns for the effect (difference between strong and weak), then expressing it as a percentage.
newdata
```



```{r}
#What is the probability that Strong is greater than weak?
newdata %>% summarize(Prob = sum(PEff>0)/n()) #99.8%

#What is the probability that Strong is 20% greater than weak?
newdata %>% summarize(Prob = sum(PEff>20)/n()) #83.3%

#On average, how different are Strong and Weak?
newdata %>% dplyr::select(Eff, PEff) %>% median_hdi #on average we have a 28.5% different. It could be as low as 8.5% and as high as 48%

```

# Summary figures

# References
