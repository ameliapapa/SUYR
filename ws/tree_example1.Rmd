---
title: "Regression Trees Part1"
author: "Murray Logan"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    collapse: no
    df_print: paged
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: textmate
    theme: spacelab
    toc: yes
    toc_float: yes
    css: ../resources/style.css
  pdf_document:
    df_print: default
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: tango
    latex_engine: xelatex
    number_sections: yes
    toc_depth: 2
  word_document:
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: tango
    toc: yes
    toc_depth: 2
output_dir: "docs"
documentclass: article
fontsize: 12pt
mainfont: Arial
mathfont: LiberationMono
monofont: DejaVu Sans Mono
classoption: a4paper
bibliography: ../resources/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

# Preparations

Load the necessary libraries

```{r libraries, results='markdown', eval=TRUE}
library(gbm)         #for gradient boosted models. getting lots of trees, each one shrunk and explains only a tiny amount. loads of them explain tons
library(car)         
library(dismo)
library(pdp)
library(ggfortify)   #coverts from one format to another. 
library(randomForest)
library(tidyverse)   #its worth putting it last
```

# Scenario

Abalone are an important aquaculture shell fish that are farmed
for both their meat and their shells.  Abalone can live up to 50 
years, although their longevity is known to be influenced by a 
range of environmental factors.  Traditionally, abalone are aged
by counting thier growth rings, however, this method is very
laborious and expensive.  Hence a study was conducted in which abalone
growth ring counts were matched up with a range of other more easily
measured physical characteristics (such as shell dimesions and weights)
in order to see if any of these other parameters could be used as
proxies for the number of growth rings (or age).

![abalone](../resources/abalone.jpg){width="251" height="290"}

Format of abalone.csv data file


# Read in the data

```{r readData, results='markdown', eval=TRUE}
library(readr)
abalone = read_csv('../data/abalone.csv', trim_ws=TRUE)
glimpse(abalone)
```
Which of these variables predict the age of abalone the best?

```{r}
abalone = abalone%>% mutate(SEX = factor(SEX)) 
```


# Exploratory data analysis
```{r}
ggplot(abalone) +
  geom_point(aes(y=AGE, x = RINGS))
```

```{r}
scatterplotMatrix(~RINGS + SEX + LENGTH + DIAMETER + HEIGHT+ WHOLE_WEIGHT + MEAT_WEIGHT + GUT_WEIGHT + SHELL_WEIGHT, data = abalone)
# It looks at the trends between the response and the predictors. 
#SEX is categorical, so we don't want to impose any monitonical relationship
#LENGTH is positive
```

- Some of the predictors are correlated -> HOW?
- RINGS -> POISSON -> determines the 'lost' function
                      Because you're only looking for the lost function, you don't need to worry about NB
                      
- You don't need to worry about the distribution of the predictors. The predictors never need to be transformed. It doesn't matter how big the gaps are, the data is going to be split anyway.
- You don't need to scale variables...
- You don't need to worry about linearity or homogeneity of variance...

# Subsetting the data

```{r fitModel, results='markdown', eval=TRUE}
set.seed(123)# sets a random seed, that is reproducable. 123rd point in the list of random numbers. 

nrow(abalone) #I'm curious to know how many rows are in the data, to know how to divide the test and training tests
i = sample(1:nrow(abalone), 100,replace=FALSE) #withhold 100 values to test precision
#sample is a function that randomly samples, and we've asked to sample without replacement, so we don't get duplicates
i %>% head

abalone.train = abalone[-i,] #take abalone data, and don't include the row with the "i" subset
#include all but... (negative means don't include these rows). Subsets by using indices. A dataset has rows and columns, and you can infer to each of those. 
#abalone[1,] first row, all columns
#abalone[,1] first column, all rows
abalone.test = abalone[i,] # only include "i" subset
```


## Fit the model
```{r fitModel, results='markdown', eval=T, cache=T}
#cache -> everytime you're kniting, it saves the long chunks, and it doens't do it again and again.
abalone.gbm = gbm(RINGS ~ SEX + LENGTH + DIAMETER + HEIGHT + #declare gradient boosted model
                      WHOLE_WEIGHT + MEAT_WEIGHT + GUT_WEIGHT + #predictors
                      SHELL_WEIGHT, 
                  data=abalone.train, #where predictors come from
                  distribution='poisson', #determines the loss function
                  var.monotone=c(0,1,1,1,1,1,1,1), #SEX is categorical, so we don't impose any monitonical relationship, but the rest of the responses are positive
                  n.trees=10000, #we determine later, if this is a good number
                  interaction.depth=5, #5 split levels, you inspect there might be 3 way interactions. If there's 7,you potentially support 5 way interactions. This is purely observations from others. Each additional depth slows down the model
                  bag.fraction=0.5, #on any give tree it will use a random 50% of the data. 
                  shrinkage=0.01, #the learning rate, the shrinkage. the lower the number, the slower it learns, and the more tree you need to learn the stuff. If you decrease shrinkage, you should increase number of trees.
                  train.fraction=1, #use all of the data
                  cv.folds=3) #use 3 CV folds. It will repeat each tree 3 times. The more you do the better, but there needs to be a balance with how long it is going to take.

``` 


```{}
(best.iter = gbm.perf(abalone.gbm,method='OOB')) #out of bag
(best.iter = gbm.perf(abalone.gbm,method='cv'))
#it stored the result as a variable, when we ask it 

summary(abalone.gbm, n.trees=best.iter)
```

```{}
plot(abalone.gbm, 8, n.tree=best.iter)

par(mfrow=c(2,4))
for (i in 1:8) plot(abalone.gbm, i, n.tree=best.iter)

library(pdp)
library(ggfortify)
## recursive indicates that a weighted tree traversal method described by Friedman 2001 (which is very fast)
## should be used (only works for gbm).  Otherwise a slower brute force method is used.
## if want to back transform - need to use brute force.
abalone.gbm %>% partial(pred.var='SHELL_WEIGHT',
                        n.trees=best.iter) %>%
    autoplot()
abalone.gbm %>% partial(pred.var='SHELL_WEIGHT',
                        n.trees=best.iter,
                        recursive=FALSE,
                        inv.link=exp) %>%
    autoplot()

abalone.gbm %>% partial(pred.var=c('SHELL_WEIGHT','MEAT_WEIGHT'),
                        n.trees=best.iter) %>%
    autoplot()

grid.arrange(
    abalone.gbm %>% partial(pred.var='SHELL_WEIGHT', n.trees=best.iter,
                            recursive=FALSE,inv.link=exp) %>%
    autoplot,
    abalone.gbm %>% partial(pred.var='MEAT_WEIGHT', n.trees=best.iter,
                            recursive=FALSE,inv.link=exp) %>%
    autoplot)
##note, these are still on the log scale

predict(abalone.gbm,newdata=abalone.test, n.tree=best.iter, type='response')

newdata = abalone.train

newdata = cbind(abalone.train,
    fit=predict(abalone.gbm,newdata=abalone.train, n.tree=best.iter, type='response'))
ggplot(newdata, aes(y=fit, x=RINGS)) + geom_point()

newdata = abalone.test %>%
    mutate(fit=predict(abalone.gbm, newdata=abalone.test,
                       n.tree=best.iter, type='response'))
ggplot(newdata, aes(y=fit, x=RINGS)) + geom_point()
    
summary(lm(fit ~ RINGS, data=newdata))

newdata.1 = cbind(abalone.test,
                  fit=predict(abalone.gbm, newdata=abalone.test, n.tree=best.iter, type='response'))
ggplot(newdata, aes(y=fit, x=RINGS)) + geom_point() +
    geom_point(data=newdata.1, color='red', size=5)



##Computes Friedman's H-statistic to assess the strength of variable interactions.
interact.gbm(abalone.gbm, abalone,5:6, n.tree=best.iter)

terms <- attr(abalone.gbm$Terms,"term.labels")
abalone.int <- NULL
for (i in 1:(length(terms)-1)) {
 for (j in (i+1):length(terms)) {
   abalone.int <- rbind(abalone.int,
        data.frame(Var1=terms[i], Var2=terms[j],
  "H.stat"=interact.gbm(abalone.gbm, abalone,c(i,j),
   n.tree=best.iter)
                   ))
 }
}
abalone.int %>% arrange(-H.stat)
terms
plot(abalone.gbm, c(5,6), n.tree=best.iter)

abalone.grid = plot(abalone.gbm, c(5,6), n.tree=best.iter, return.grid=TRUE)
head(abalone.grid)

ggplot(abalone.grid, aes(y=MEAT_WEIGHT, x=WHOLE_WEIGHT)) +
    geom_tile(aes(fill=y)) +
    geom_contour(aes(z=y)) +
    scale_fill_gradientn(colors=heat.colors(10))

### gbm.step
library(dismo)
abalone.gbm1 <- gbm.step(data=abalone %>% as.data.frame, gbm.x=1:8, gbm.y=9,
                        tree.complexity=5,
                        learning.rate=0.01,
                        bag.fraction=0.5,
                        n.trees=10000,
                        family='poisson')
summary(abalone.gbm1)
gbm.plot(abalone.gbm1, n.plots=8, write.title = FALSE)
gbm.plot.fits(abalone.gbm1)
find.int <- gbm.interactions(abalone.gbm1)
summary(find.int)
find.int$rank.list
gbm.perspec(abalone.gbm1,6,5)

```





# Model validation

# Model investigation / hypothesis testing

```{r Tree1, cache=TRUE, eval=FALSE}
abalone.gbm = ...
save(abalone.gbm, file='data/abalone.gbm.RData')
```


```{r , eval=FALSE}
load(file='data/abalone.gbm.RData')
```

# Predictions

# Summary figures
