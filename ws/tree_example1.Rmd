---
title: "Regression Trees Part1"
author: "Murray Logan"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    collapse: no
    df_print: paged
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: textmate
    theme: spacelab
    toc: yes
    toc_float: yes
    css: ../resources/style.css
  pdf_document:
    df_print: default
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: tango
    latex_engine: xelatex
    number_sections: yes
    toc_depth: 2
  word_document:
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: tango
    toc: yes
    toc_depth: 2
output_dir: "docs"
documentclass: article
fontsize: 12pt
mainfont: Arial
mathfont: LiberationMono
monofont: DejaVu Sans Mono
classoption: a4paper
bibliography: ../resources/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

# Preparations

Load the necessary libraries

```{r libraries, results='markdown', eval=TRUE}
library(gbm)         #for gradient boosted models. getting lots of trees, each one shrunk and explains only a tiny amount. loads of them explain tons
library(car)         
library(dismo)
library(pdp)
library(ggfortify)   #coverts from one format to another. 
library(randomForest)
library(tidyverse)   #its worth putting it last
```

# Scenario

Abalone are an important aquaculture shell fish that are farmed
for both their meat and their shells.  Abalone can live up to 50 
years, although their longevity is known to be influenced by a 
range of environmental factors.  Traditionally, abalone are aged
by counting thier growth rings, however, this method is very
laborious and expensive.  Hence a study was conducted in which abalone
growth ring counts were matched up with a range of other more easily
measured physical characteristics (such as shell dimesions and weights)
in order to see if any of these other parameters could be used as
proxies for the number of growth rings (or age).

![abalone](../resources/abalone.jpg){width="251" height="290"}

Format of abalone.csv data file


# Read in the data

```{r readData, results='markdown', eval=TRUE}
library(readr)
abalone = read_csv('../data/abalone.csv', trim_ws=TRUE)
glimpse(abalone)
```
Which of these variables predict the age of abalone the best?

```{r}
abalone = abalone%>% mutate(SEX = factor(SEX)) 
```


# Exploratory data analysis
```{r}
ggplot(abalone) +
  geom_point(aes(y=AGE, x = RINGS))
```

```{r}
scatterplotMatrix(~RINGS + SEX + LENGTH + DIAMETER + HEIGHT+ WHOLE_WEIGHT + MEAT_WEIGHT + GUT_WEIGHT + SHELL_WEIGHT, data = abalone)
# It looks at the trends between the response and the predictors. 
#SEX is categorical, so we don't want to impose any monitonical relationship
#LENGTH is positive
```

- Some of the predictors are correlated -> HOW?
- RINGS -> POISSON -> determines the 'lost' function
                      Because you're only looking for the lost function, you don't need to worry about NB
                      
- You don't need to worry about the distribution of the predictors. The predictors never need to be transformed. It doesn't matter how big the gaps are, the data is going to be split anyway.
- You don't need to scale variables...
- You don't need to worry about linearity or homogeneity of variance...

# Subsetting the data

```{r fitModel, results='markdown', eval=TRUE}
set.seed(123)# sets a random seed, that is reproducable. 123rd point in the list of random numbers. 

nrow(abalone) #I'm curious to know how many rows are in the data, to know how to divide the test and training tests
i = sample(1:nrow(abalone), 100,replace=FALSE) #withhold 100 values to test precision
#sample is a function that randomly samples, and we've asked to sample without replacement, so we don't get duplicates
i %>% head

abalone.train = abalone[-i,] #take abalone data, and don't include the row with the "i" subset
#include all but... (negative means don't include these rows). Subsets by using indices. A dataset has rows and columns, and you can infer to each of those. 
#abalone[1,] first row, all columns
#abalone[,1] first column, all rows
abalone.test = abalone[i,] # only include "i" subset
```


## Fit the model
```{r fitModel, results='markdown', eval=T, cache=T}
#cache -> everytime you're kniting, it saves the long chunks, and it doens't do it again and again.
abalone.gbm = gbm(RINGS ~ SEX + LENGTH + DIAMETER + HEIGHT + #declare gradient boosted model
                      WHOLE_WEIGHT + MEAT_WEIGHT + GUT_WEIGHT + #predictors
                      SHELL_WEIGHT, 
                  data=abalone.train, #where predictors come from
                  distribution='poisson', #determines the loss function
                  var.monotone=c(0,1,1,1,1,1,1,1), #SEX is categorical, so we don't impose any monitonical relationship, but the rest of the responses are positive
                  n.trees=10000, #we determine later, if this is a good number
                  interaction.depth=5, #5 split levels, you inspect there might be 3 way interactions. If there's 7,you potentially support 5 way interactions. This is purely observations from others. Each additional depth slows down the model
                  bag.fraction=0.5, #on any give tree it will use a random 50% of the data. 
                  shrinkage=0.01, #the learning rate, the shrinkage. the lower the number, the slower it learns, and the more tree you need to learn the stuff. If you decrease shrinkage, you should increase number of trees.
                  train.fraction=1, #use all of the data
                  cv.folds=3) #use 3 CV folds. It will repeat each tree 3 times. The more you do the better, but there needs to be a balance with how long it is going to take.

(best.iter = gbm.perf(abalone.gbm,method='OOB')) #out of bag

(best.iter = gbm.perf(abalone.gbm,method='cv'))
#it stored the result as a variable, when we ask it 

summary(abalone.gbm, n.trees=best.iter)
#the biggest predictor of age, is shell weight. Shell weight involved 86% of the splits. 6% of the splits were HEIGHT, etc... If you add up all of the importances, they add up to 100. If they were equally important, you would expect 100/8 importance. If the relative importance is lower than 12.5, is really not important. 
```

```{r}
plot(abalone.gbm, 8, n.tree=best.iter)

par(mfrow=c(2,4))
for (i in 1:8) plot(abalone.gbm, i, n.tree=best.iter)
```

```{r}
library(pdp)
library(ggfortify)
## recursive indicates that a weighted tree traversal method described by Friedman 2001 (which is very fast)
## should be used (only works for gbm).  Otherwise a slower brute force method is used.
## if want to back transform - need to use brute force.
abalone.gbm %>% pdp::partial(pred.var='SHELL_WEIGHT',
                        n.trees=best.iter) %>% autoplot()
#tidyverse being loaded last, has a function called 'purrr' that overwrites 'partial' of pdp. We need to tell r to use partial from the package pdp, instead of purrr.

```


```{r}
#Rather than plotting the partial plot in the log scale, we perform the inverse, and put it back on our natural scale
abalone.gbm %>% pdp::partial(pred.var='SHELL_WEIGHT',
                        n.trees=best.iter,
                        recursive=FALSE,
                        inv.link=exp) %>%
    autoplot()
#if there's missing values, the whole rows goes out. Regression trees try to predict the missing data, depending on what's already there. 

#Prediction 
```

```{r}
abalone.gbm %>% pdp::partial(pred.var=c('SHELL_WEIGHT','MEAT_WEIGHT'),
                        n.trees=best.iter) %>%
    autoplot() #heat map
```

```{r}
grid.arrange(
    abalone.gbm %>% pdp::partial(pred.var='SHELL_WEIGHT', n.trees=best.iter,
                            recursive=FALSE,inv.link=exp) %>%
    autoplot,
    abalone.gbm %>% pdp::partial(pred.var='HEIGHT', n.trees=best.iter,
                            recursive=FALSE,inv.link=exp) %>%
    autoplot) 

#Alternatively:
#g1 = abalone.gbm %>% pdp::partial(pred.var='SHELL_WEIGHT', n.trees=best.iter,
#                            recursive=FALSE,inv.link=exp) %>%
#    autoplot
#g2 = abalone.gbm %>% pdp::partial(pred.var='HEIGHT', n.trees=best.iter,
#                            recursive=FALSE,inv.link=exp) %>%
#    autoplot
#grid.arrange(g1,g2, nrow = 1)
```


#Predictions

```{r}
predict(abalone.gbm,newdata=abalone.test, n.tree=best.iter, type='response')
#we've largely ignored the predict function, because it often doesn't provide standard errors to get CI's. 
#But if we just wanted predicted values, it is fine
```

```{r}
cbind(
  predict(abalone.gbm, newdata = abalone.test, n.tree = best.iter, type = 'response'), abalone.test$RINGS
)
```
```{r}
cbind(
  predict(abalone.gbm, newdata = abalone.test, n.tree = best.iter, type = 'response'), abalone.test$RINGS
) %>% plot #Predicted in x, and observed in y
```



```{r} 
##Predictions are on the x axis, instead of the y axis
newdata = abalone.test

newdata = cbind(abalone.test,
    fit=predict(abalone.gbm,newdata=abalone.test, n.tree=best.iter, type='response'))
ggplot(newdata, aes(y=fit, x=RINGS)) + geom_point()

```


```{r}
##SAME HERE
newdata = abalone.test %>%
    mutate(fit=predict(abalone.gbm, newdata=abalone.test,
                       n.tree=best.iter, type='response'))
ggplot(newdata, aes(y=fit, x=RINGS)) + geom_point()
    
```

#Confidence intervals on regression trees:
- We want to project the uncertainty in our outcomes
1. If we can get access to what the CV data saw in every tree, then we can get uncertainty. Because the tree was repeated three times. Unfortunately, gbm doesn't giv us this. ABT gets access to these bits, but Glenn De'ath had a falling out with CRAN, so you can contact him
2. Bootstrapping -> 


```{r}
summary(lm(fit ~ RINGS, data=newdata))

```

```{r}
newdata.1 = cbind(abalone.test,
                  fit=predict(abalone.gbm, newdata=abalone.test, n.tree=best.iter, type='response'))
ggplot(newdata, aes(y=fit, x=RINGS)) + geom_point() +
    geom_point(data=newdata.1, color='red', size=5)
```


```{r}

##Computes Friedman's H-statistic to assess the strength of variable interactions.
interact.gbm(abalone.gbm, abalone,5:6, n.tree=best.iter)
#[1] 2.02862e-13 Interaction between 4 and 8
interact.gbm(abalone.gbm, abalone,c(4,8), n.tree=best.iter)
#[1] 0.06572818 Interaction between 4 and 8
```

```{r}
terms <- attr(abalone.gbm$Terms,"term.labels") #terms are the names of your predictors. 
terms
#we use this for our loop
```


```{r}
abalone.int <- NULL #to collect interactions, we need the variable to be empty

#we're looking for every two way interaction
for (i in 1:(length(terms)-1)) { #'i' is used as a counter (iteration number), 'in' means the first time we run this loop i is equal to 1
#we want it to go from 1-7 (length=8). 
 for (j in (i+1):length(terms)) { #another nested loop inside, so we need a new counter 'j'. we want it to start one beyond what we had for i. It starts at whatever i was, and will go until 8. 
   print(paste('i=',i, 'Name = ', terms[i]))
   print(paste('j=',j, 'Name = ', terms[j]))
   abalone.int <- rbind(abalone.int,
        data.frame(Var1=terms[i], Var2=terms[j],
  "H.stat"=interact.gbm(abalone.gbm, abalone,c(i,j),
   n.tree=best.iter)
                   ))
 }
}
```


```{r}
abalone.int %>% arrange(-H.stat) #we put them in order to the highest is in the top. This way we see which interaction was the strongest.
#The strongets interaction was between SEX and HEIGHT, but we decided they were not very useful
```

```{r}
terms
plot(abalone.gbm, c(5,6), n.tree=best.iter)
```


```{r}
abalone.grid = plot(abalone.gbm, c(5,6), n.tree=best.iter, return.grid=TRUE)
head(abalone.grid)

ggplot(abalone.grid, aes(y=MEAT_WEIGHT, x=WHOLE_WEIGHT)) +
    geom_tile(aes(fill=y)) +
    geom_contour(aes(z=y)) +
    scale_fill_gradientn(colors=heat.colors(10))
```


```{r}
### gbm.step
library(dismo)
abalone.gbm1 <- gbm.step(data=abalone %>% as.data.frame, gbm.x=1:8, gbm.y=9,
                         #it needs a dataframe and can't use the tibble
                         #our predictors are in column 1-8 and the response                           is in column 9
                        tree.complexity=5,
                        learning.rate=0.01,
                        bag.fraction=0.5,
                        n.trees=10000,
                        family='poisson')
summary(abalone.gbm1)
#In the presence and absence of each variable, SHELL_WEIGHT and MEAT_WEIGHT show most influential
```

ALternatively
for (i in c(0.1,0.01)) {
abalone.gbm1 <- gbm.step(data=abalone %>% as.data.frame, gbm.x=1:8, gbm.y=9,
                         #it needs a dataframe and can't use the tibble
                         #our predictors are in column 1-8 and the response                           is in column 9
                        tree.complexity=5,
                        learning.rate=0.01,
                        bag.fraction=0.5,
                        shrinkage = i
                        n.trees=10000,
                        family='poisson')
}


### RANDOM FOREST
- A boosted tree subsets the rows. It always has the same number of predictors, but it has a different combination of rows each time
- A random forest, uses the same # of rows, but different # of predictors
- Individually they have different importances than in boosted tree. In random forest, they will get half the importance.
```{r}
abalone.rf = randomForest(RINGS ~ SEX + LENGTH+ DIAMETER + HEIGHT +
                            WHOLE_WEIGHT + MEAT_WEIGHT + GUT_WEIGHT +
                            SHELL_WEIGHT,
                          data = abalone.train,
                          distributions = 'poisson',
                          var.monotone = c(0,1,1,1,1,1,1,1),
                          n.trees = 10000,
                          interaction.depth = 5,
                          bag.fraction = 0.5,
                          shrinkage = i,
                          train.fraction = 1,
                          cv.folds = 3)
library(randomForest)
abalone.rf = randomForest(RINGS ~ SEX + LENGTH + DIAMETER + HEIGHT +
                            WHOLE_WEIGHT + MEAT_WEIGHT + GUT_WEIGHT + 
                            SHELL_WEIGHT, data = abalone.train, importance = T, ntree = 10000)

abalone.imp = importance(abalone.rf)
100*abalone.imp/sum(abalone.imp)
#If you were to even out the importances with random forest, SHELL_WEIGHT doesn't stick out as a massively important predictor. Because they are highly correlated, they are evened out. SEX wasn't correlated to any of the other, so a weak importance was observed. 
```

```{r}
library(pdp)
abalone.rf %>% pdp::partial('SHELL_WEIGHT') %>% autoplot
```


```{r}
gbm.plot(abalone.gbm1, n.plots=8, write.title = FALSE)
gbm.plot.fits(abalone.gbm1)
find.int <- gbm.interactions(abalone.gbm1)
summary(find.int)
find.int$rank.list
gbm.perspec(abalone.gbm1,6,5)

```


# Model validation

# Model investigation / hypothesis testing

```{r Tree1, cache=TRUE, eval=FALSE}
abalone.gbm = ...
save(abalone.gbm, file='data/abalone.gbm.RData')
```


```{r , eval=FALSE}
load(file='data/abalone.gbm.RData')
```

# Predictions

# Summary figures
